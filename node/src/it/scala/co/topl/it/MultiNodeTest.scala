package co.topl.it

import co.topl.it.util._
import com.typesafe.config.ConfigFactory
import io.circe.syntax._
import org.scalatest.concurrent.PatienceConfiguration.Timeout
import org.scalatest.concurrent.ScalaFutures
import org.scalatest.freespec.AnyFreeSpec
import org.scalatest.matchers.should.Matchers
import org.scalatest.{EitherValues, Inspectors}

import scala.concurrent.Future
import scala.concurrent.duration._

class MultiNodeTest
    extends AnyFreeSpec
    with Matchers
    with IntegrationSuite
    with ScalaFutures
    with EitherValues
    with Inspectors {

  import system.dispatcher

  val nodeCount: Int = 3
  // Values close to 0.0 indicate a strict/tight fairness threshold
  val blockDistributionTolerance: Double = 0.99d
  val forgeDuration: FiniteDuration = 5.minutes
  val seed: String = "MultiNodeTest" + System.currentTimeMillis()

  override implicit val patienceConfig: PatienceConfig = PatienceConfig(2.seconds)

  "Multiple nodes can forge blocks with roughly equal distribution" in {

    val nodes = createAndStartNodes()

    assignForgingAddress(nodes)

    // Fetch the initial count of blocks generated by address.  Because forging has not started, only a single
    // genesis block should be assigned to a single address
    val initialGeneratorCounts: Map[String, Map[String, Long]] =
      Future
        .traverse(nodes)(node => node.Debug.generators().map(node.containerId -> _.value))
        .futureValue
        .toMap

    initialGeneratorCounts.foreach { case (containerId, generatorCounts) =>
      logger.info(
        s"Initial block generator counts for containerId=$containerId:\n${generatorCounts.asJson.spaces2SortKeys}"
      )
    }

    forAll(initialGeneratorCounts.values) { counts =>
      counts should have size 1
      counts.head._2 shouldBe 1L
      forAll(initialGeneratorCounts.values)(counts should contain theSameElementsAs _)
    }

    val genesisAddress = initialGeneratorCounts.head._2.head._1

    // Now instruct the nodes to start forging
    nodes.foreach(_.Admin.startForging().futureValue.value)

    logger.info(s"Waiting $forgeDuration for forging")
    Thread.sleep(forgeDuration.toMillis)

    // Verify that each node has forged a roughly equal number of blocks according to their own "myBlocks" information
    val forgeCounts =
      Future
        .traverse(nodes)(node => node.Debug.myBlocks().map(node.containerId -> _.value))
        .futureValue
        .toMap

    forgeCounts.foreach { case (containerId, count) =>
      logger.info(s"myBlocks forging count=$count containerId=$containerId")
    }

    forAll(forgeCounts.values)(_ should be > 0L)

    val mean = forgeCounts.values.sum / forgeCounts.size

    forAll(forgeCounts.values)(_ should be(mean +- (mean * blockDistributionTolerance).toLong))

    // And now verify that the nodes have shared understanding of how many blocks their peers have forged
    val finalGeneratorCounts: Map[String, Map[String, Long]] =
      Future
        .traverse(nodes)(node => node.Debug.generators().map(node.containerId -> _.value))
        .futureValue
        .toMap

    finalGeneratorCounts.foreach { case (containerId, generatorCounts) =>
      logger.info(
        s"Final block generator counts for containerId=$containerId:\n${generatorCounts.asJson.spaces2SortKeys}"
      )
    }

    forAll(finalGeneratorCounts.values) { counts =>
      counts should have size (nodeCount + 1)
      counts(genesisAddress) shouldBe 1L
      forAll(finalGeneratorCounts.values)(counts should contain theSameElementsAs _)
    }

    val headGeneratorCounts =
      finalGeneratorCounts.head._2 - genesisAddress

    headGeneratorCounts should have size nodes.size

    // And verify again that the block distribution was fair
    val generatorCountMean = headGeneratorCounts.values.sum / headGeneratorCounts.size

    forAll(headGeneratorCounts.values)(_ should be(mean +- (generatorCountMean * blockDistributionTolerance).toLong))
  }

  /** The genesis block contains pre-loaded addresses, one for each of our test nodes.  Assign
    * each test node to a single address by locking each node out of all-but-one address, where the assigned
    * address is determined by the node's index in the given list.
    */
  def assignForgingAddress(nodes: List[BifrostDockerNode]): Unit = {
    val allAddresses: Map[String, List[String]] =
      Future
        .traverse(nodes)(node => node.Admin.listOpenKeyfiles().map(node.containerId -> _.value))
        .futureValue(Timeout(10.seconds))
        .toMap

    forAll(allAddresses.values)(addresses => forAll(allAddresses.values)(addresses should contain theSameElementsAs _))

    val addressList = allAddresses.head._2

    nodes.zipWithIndex.foreach { case (node, index) =>
      addressList.zipWithIndex.foreach {
        case (address, addressIndex) if addressIndex != index =>
          node.Admin.lockKeyfile(address).futureValue.value
        case _ =>
      }
    }
  }

  /** Launches a group of nodes, all on the same Docker network and sharing the same seed.  Forging
    * is disabled on startup.
    */
  def createAndStartNodes(): List[BifrostDockerNode] = {
    val nodeNames = List.tabulate(nodeCount)("bifrostMultiNode" + _)

    val config =
      ConfigFactory.parseString(
        raw"""bifrost.network.knownPeers = ${nodeNames.map(n => s"$n:${BifrostDockerNode.NetworkPort}").asJson}
             |bifrost.rpcApi.namespaceSelector.debug = true
             |bifrost.forging.privateTestnet.numTestnetAccts = $nodeCount
             |bifrost.forging.privateTestnet.genesisSeed = "$seed"
             |bifrost.forging.forgeOnStartup = false
             |""".stripMargin
      )

    val nodes = nodeNames.map(dockerSupport.createNode(_, "MultiNodeTest"))

    nodes.foreach(_.reconfigure(config))
    nodes.foreach(_.start())

    Thread.sleep(20.seconds.toMillis)

    // Startup may take a bit longer in multi-node tests because they need to synchronize first
    Future
      .traverse(nodes)(_.waitForStartup().map(_.value))
      .futureValue(Timeout(60.seconds))

    nodes
  }

}
